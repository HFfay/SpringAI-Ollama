spring.application.name=SpringAI-Ollama
spring.ai.ollama.base-url=http://127.0.0.1:11434/
spring.ai.ollama.chat.options.model=qwen:14b
# Sets the size of the context window used to generate the next token.
#spring.ai.ollama.chat.options.num-ctx=
# The temperature of the model. Increasing the temperature will make the model answer more creatively.
spring.ai.ollama.chat.options.temperature=0.7

spring.ai.ollama.embedding.options.model=qwen:14b
# Sets the size of the context window used to generate the next token.
#spring.ai.ollama.embedding.options.num-ctx=
# Sets the number of threads to use during computation. By default, Ollama will detect this for optimal performance. It is recommended to set this value to the number of physical CPU cores your system has (as opposed to the logical number of cores).
#spring.ai.ollama.embedding.options.num-thread